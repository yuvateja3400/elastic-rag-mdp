# Elastic RAG MDP

A modular **Retrieval-Augmented Generation (RAG)** system built with **Elasticsearch**, **FastAPI**, and **Streamlit**, using open-source LLMs.  
This project was developed as part of an internship assignment and follows industry standards for modular design, reproducibility, and deployment.

---

## 🚀 Features
- Ingests PDFs from Google Drive or local folder
- Indexes text chunks into Elasticsearch with:
  - **BM25** (keyword search)
  - **ELSER** (sparse embeddings)
  - **Dense embeddings** (Sentence Transformers)
- Hybrid retrieval (Reciprocal Rank Fusion)
- Answer generation with open LLMs (HuggingFace / Ollama)
- FastAPI backend (`/query`, `/ingest`, `/healthz`)
- Streamlit UI for interactive queries
- Kibana dashboard for inspecting indices & embeddings
- Modular Python package structure
- Dockerized setup for reproducibility

---

## 📂 Project Structure
```
elastic-rag-mdp/
├── app/
│   ├── config.py       # centralized settings
│   ├── ingestion.py    # PDF text extraction + chunking
│   ├── indexing.py     # Elasticsearch index + insert
│   ├── retrieval.py    # ELSER, dense, BM25, hybrid
│   ├── generation.py   # LLM prompt + response
│   ├── guardrails.py   # safety and grounded answers
│   ├── api.py          # FastAPI routes
│   └── ui.py           # Streamlit app
├── tests/              # unit tests
├── data/               # PDFs
├── Dockerfile
├── docker-compose.yml
├── requirements.txt
├── .env.example
└── README.md
```

---

## ⚙️ Setup

### 1. Clone repo
```bash
git clone https://github.com/<your-username>/elastic-rag-mdp.git
cd elastic-rag-mdp
```

### 2. Create virtual environment
```bash
python -m venv .venv
source .venv/bin/activate   # Mac/Linux
# .venv\Scripts\activate    # Windows
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

### 4. Configure environment
Copy `.env.example` → `.env` and update values if needed:
```bash
cp .env.example .env
```

---

## 🐳 Docker Setup

### Start services
```bash
docker compose up --build
```

- Elasticsearch → [http://localhost:9200](http://localhost:9200)  
- Kibana → [http://localhost:5601](http://localhost:5601)  
- FastAPI → [http://localhost:8000/docs](http://localhost:8000/docs)  
- Streamlit → [http://localhost:8501](http://localhost:8501)  

---

## 🔍 Example Workflow

1. **Ingest PDFs**  
   Place your documents inside `data/pdfs/`.  
   Start services:  
   ```bash
   docker compose up --build
   ```

2. **Index Documents**  
   - Trigger ingestion via API (FastAPI → Streamlit will use this under the hood):  
     ```bash
     curl -X POST "http://localhost:8000/ingest"
     ```
   - Elasticsearch will store the chunks with BM25, ELSER, and dense embeddings.

3. **Ask Questions**  
   - Open **Streamlit UI** → [http://localhost:8501](http://localhost:8501)  
   - Type your question in the input box.  
   - The answer is generated by the LLM and citations are shown (title + snippet).  

4. **Inspect Index and Embeddings (for debugging)**  
   - Open **Kibana** → [http://localhost:5601](http://localhost:5601)  
   - Explore the `rag_documents` index.  
   - You can see stored fields:  
     - `text` (raw chunk)  
     - `text_expansion` (ELSER sparse vector)  
     - `dense_vector` (dense embeddings)  
     - metadata (filename, chunk_id, etc.)  

👉 In practice:  
- **Streamlit = user-facing interface for Q&A**  
- **Kibana = developer-facing interface to validate embeddings and search quality**  

---

## 🧪 Testing
Run unit tests:
```bash
pytest tests/
```

---

## 📜 License
MIT License
